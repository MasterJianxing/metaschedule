2023-04-11 10:46:37 [INFO] [task_scheduler.cc:160] Initializing Task #10: "fused_nn_conv2d_add_nn_relu_2"
2023-04-11 10:46:37 [INFO] [task_scheduler.cc:35] 
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(p0: T.Buffer[(T.int64(1), T.int64(256), T.int64(56), T.int64(56)), "float16"], p1: T.Buffer[(T.int64(64), T.int64(256), T.int64(1), T.int64(1)), "float16"], p2: T.Buffer[(T.int64(1), T.int64(64), T.int64(1), T.int64(1)), "float16"], T_relu: T.Buffer[(T.int64(1), T.int64(64), T.int64(56), T.int64(56)), "float16"]):
        # function attr dict
        T.func_attr({"global_symbol": "main", "tir.noalias": True})
        # body
        # with T.block("root")
        pad_temp = T.alloc_buffer([T.int64(1), T.int64(256), T.int64(56), T.int64(56)], dtype="float16")
        conv2d_nchw = T.alloc_buffer([T.int64(1), T.int64(64), T.int64(56), T.int64(56)], dtype="float16")
        T_add = T.alloc_buffer([T.int64(1), T.int64(64), T.int64(56), T.int64(56)], dtype="float16")
        for i0, i1, i2, i3 in T.grid(T.int64(1), T.int64(256), T.int64(56), T.int64(56)):
            with T.block("pad_temp"):
                v_i0, v_i1, v_i2, v_i3 = T.axis.remap("SSSS", [i0, i1, i2, i3])
                T.reads(p0[v_i0, v_i1, v_i2, v_i3])
                T.writes(pad_temp[v_i0, v_i1, v_i2, v_i3])
                pad_temp[v_i0, v_i1, v_i2, v_i3] = p0[v_i0, v_i1, v_i2, v_i3]
        for nn, ff, yy, xx, rc, ry, rx in T.grid(T.int64(1), T.int64(64), T.int64(56), T.int64(56), T.int64(256), T.int64(1), T.int64(1)):
            with T.block("conv2d_nchw"):
                v_nn, v_ff, v_yy, v_xx, v_rc, v_ry, v_rx = T.axis.remap("SSSSRRR", [nn, ff, yy, xx, rc, ry, rx])
                T.reads(pad_temp[v_nn, v_rc, v_yy + v_ry, v_xx + v_rx], p1[v_ff, v_rc, v_ry, v_rx])
                T.writes(conv2d_nchw[v_nn, v_ff, v_yy, v_xx])
                with T.init():
                    conv2d_nchw[v_nn, v_ff, v_yy, v_xx] = T.float16(0)
                conv2d_nchw[v_nn, v_ff, v_yy, v_xx] = conv2d_nchw[v_nn, v_ff, v_yy, v_xx] + pad_temp[v_nn, v_rc, v_yy + v_ry, v_xx + v_rx] * p1[v_ff, v_rc, v_ry, v_rx]
        for ax0, ax1, ax2, ax3 in T.grid(T.int64(1), T.int64(64), T.int64(56), T.int64(56)):
            with T.block("T_add"):
                v_ax0, v_ax1, v_ax2, v_ax3 = T.axis.remap("SSSS", [ax0, ax1, ax2, ax3])
                T.reads(conv2d_nchw[v_ax0, v_ax1, v_ax2, v_ax3], p2[v_ax0, v_ax1, T.int64(0), T.int64(0)])
                T.writes(T_add[v_ax0, v_ax1, v_ax2, v_ax3])
                T_add[v_ax0, v_ax1, v_ax2, v_ax3] = conv2d_nchw[v_ax0, v_ax1, v_ax2, v_ax3] + p2[v_ax0, v_ax1, T.int64(0), T.int64(0)]
        for ax0, ax1, ax2, ax3 in T.grid(T.int64(1), T.int64(64), T.int64(56), T.int64(56)):
            with T.block("T_relu"):
                v_ax0, v_ax1, v_ax2, v_ax3 = T.axis.remap("SSSS", [ax0, ax1, ax2, ax3])
                T.reads(T_add[v_ax0, v_ax1, v_ax2, v_ax3])
                T.writes(T_relu[v_ax0, v_ax1, v_ax2, v_ax3])
                T_relu[v_ax0, v_ax1, v_ax2, v_ax3] = T.max(T_add[v_ax0, v_ax1, v_ax2, v_ax3], T.float16(0))
    

2023-04-11 10:46:37 [INFO] [multi_level_tiling_tensor_core.cc:212] Sketch 0: tensorizing with wmma_sync_16x16x16_f16f16f16_trans
2023-04-11 10:46:37 [INFO] [multi_level_tiling_tensor_core.cc:212] Sketch 1: tensorizing with wmma_sync_16x16x16_f16f16f16
2023-04-11 10:46:37 [INFO] [task_scheduler.cc:164] Total 2 design space(s) generated
2023-04-11 10:46:37 [INFO] [task_scheduler.cc:170] Design space #0:
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(p0: T.Buffer[(T.int64(1), T.int64(256), T.int64(56), T.int64(56)), "float16"], p1: T.Buffer[(T.int64(64), T.int64(256), T.int64(1), T.int64(1)), "float16"], p2: T.Buffer[(T.int64(1), T.int64(64), T.int64(1), T.int64(1)), "float16"], T_relu: T.Buffer[(T.int64(1), T.int64(64), T.int64(56), T.int64(56)), "float16"]):
        # function attr dict
        T.func_attr({"global_symbol": "main", "tir.noalias": True})
        # body
        with T.block("root"):
            T.reads()
            T.writes()
            T.block_attr({"meta_schedule.unroll_explicit":0})
            conv2d_nchw_reindex_shared = T.alloc_buffer([T.int64(3136), T.int64(64)], dtype="float16", scope="shared")
            conv2d_nchw_reindex_shared_wmma_accumulator = T.alloc_buffer([T.int64(3136), T.int64(64)], dtype="float16", scope="wmma.accumulator")
            pad_temp_reindex_shared = T.alloc_buffer([T.int64(3136), T.int64(256)], dtype="float16", scope="shared")
            p1_reindex_shared = T.alloc_buffer([T.int64(1), T.int64(1), T.int64(256), T.int64(64)], dtype="float16", scope="shared")
            pad_temp_reindex_shared_wmma_matrix_a = T.alloc_buffer([T.int64(3136), T.int64(256)], dtype="float16", scope="wmma.matrix_a")
            p1_reindex_shared_wmma_matrix_b = T.alloc_buffer([T.int64(1), T.int64(1), T.int64(256), T.int64(64)], dtype="float16", scope="wmma.matrix_b")
            for ax2_0_0_ax3_0_0_fused in T.thread_binding(T.int64(1), thread="blockIdx.y"):
                for ax2_0_1_ax3_0_1_fused in T.thread_binding(T.int64(7), thread="blockIdx.x"):
                    for ax2_0_2_ax3_0_2_fused in T.thread_binding(T.int64(14), thread="threadIdx.y"):
                        for ax0_0, ax1_0, ax4_0_0 in T.grid(T.int64(1), T.int64(1), T.int64(1)):
                            for ax0_ax1_fused in T.serial(T.int64(114688)):
                                with T.block("pad_temp_reindex_shared"):
                                    v0 = T.axis.spatial(T.int64(3136), ax2_0_1_ax3_0_1_fused * T.int64(448) + ax0_ax1_fused // T.int64(256))
                                    v1 = T.axis.spatial(T.int64(256), ax0_ax1_fused % T.int64(256))
                                    T.reads(p0[v0 // T.int64(3136), v1, v0 // T.int64(56), v0 % T.int64(56)])
                                    T.writes(pad_temp_reindex_shared[v0, v1])
                                    T.block_attr({"buffer_dim_align":[[0, 0, 32, 8]], "meta_schedule.cooperative_fetch":8})
                                    pad_temp_reindex_shared[v0, v1] = p0[v0 // T.int64(3136), v1, v0 // T.int64(56), v0 % T.int64(56)]
                            for ax0_ax1_ax2_ax3_fused in T.serial(T.int64(16384)):
                                with T.block("p1_reindex_shared"):
                                    v0 = T.axis.spatial(T.int64(1), T.int64(0))
                                    v1 = T.axis.spatial(T.int64(1), T.int64(0))
                                    v2 = T.axis.spatial(T.int64(256), ax0_ax1_ax2_ax3_fused // T.int64(64))
                                    v3 = T.axis.spatial(T.int64(64), ax0_ax1_ax2_ax3_fused % T.int64(64))
                                    T.reads(p1[v3, v2, v0, v1])
                                    T.writes(p1_reindex_shared[v0, v1, v2, v3])
                                    T.block_attr({"buffer_dim_align":[[0, 2, 32, 8]], "meta_schedule.cooperative_fetch":4})
                                    p1_reindex_shared[v0, v1, v2, v3] = p1[v3, v2, v0, v1]
                            for ax0_1, ax1_1, ax4_0_1 in T.grid(T.int64(1), T.int64(1), T.int64(8)):
                                for ax0_0_1, ax1_0_1 in T.grid(T.int64(2), T.int64(2)):
                                    with T.block("pad_temp_reindex_shared_wmma.matrix_a_o"):
                                        v0_o = T.axis.spatial(T.int64(196), ax2_0_1_ax3_0_1_fused * T.int64(28) + ax2_0_2_ax3_0_2_fused * T.int64(2) + ax0_0_1)
                                        v1_o = T.axis.spatial(T.int64(16), ax4_0_1 * T.int64(2) + ax1_0_1)
                                        T.reads(pad_temp_reindex_shared[v0_o * T.int64(16) : v0_o * T.int64(16) + T.int64(16), v1_o * T.int64(16) : v1_o * T.int64(16) + T.int64(16)])
                                        T.writes(pad_temp_reindex_shared_wmma_matrix_a[v0_o * T.int64(16) : v0_o * T.int64(16) + T.int64(16), v1_o * T.int64(16) : v1_o * T.int64(16) + T.int64(16)])
                                        T.block_attr({"meta_schedule.auto_tensorize":"wmma_load_16x16x16_f16_a"})
                                        for ax0_1_1, ax1_1_1 in T.grid(T.int64(16), T.int64(16)):
                                            with T.block("pad_temp_reindex_shared_wmma.matrix_a"):
                                                v0_i, v1_i = T.axis.remap("SS", [ax0_1_1, ax1_1_1])
                                                T.reads(pad_temp_reindex_shared[v0_o * T.int64(16) + v0_i, v1_o * T.int64(16) + v1_i])
                                                T.writes(pad_temp_reindex_shared_wmma_matrix_a[v0_o * T.int64(16) + v0_i, v1_o * T.int64(16) + v1_i])
                                                pad_temp_reindex_shared_wmma_matrix_a[v0_o * T.int64(16) + v0_i, v1_o * T.int64(16) + v1_i] = pad_temp_reindex_shared[v0_o * T.int64(16) + v0_i, v1_o * T.int64(16) + v1_i]
                                for ax0, ax1, ax2_0, ax3_0 in T.grid(T.int64(1), T.int64(1), T.int64(2), T.int64(4)):
                                    with T.block("p1_reindex_shared_wmma.matrix_b_o"):
                                        v0, v1 = T.axis.remap("SS", [ax0, ax1])
                                        v2_o = T.axis.spatial(T.int64(16), ax4_0_1 * T.int64(2) + ax2_0)
                                        v3_o = T.axis.spatial(T.int64(4), ax3_0)
                                        T.reads(p1_reindex_shared[v0, v1, v2_o * T.int64(16) : v2_o * T.int64(16) + T.int64(16), v3_o * T.int64(16) : v3_o * T.int64(16) + T.int64(16)])
                                        T.writes(p1_reindex_shared_wmma_matrix_b[v0, v1, v2_o * T.int64(16) : v2_o * T.int64(16) + T.int64(16), v3_o * T.int64(16) : v3_o * T.int64(16) + T.int64(16)])
                                        T.block_attr({"meta_schedule.auto_tensorize":"wmma_load_16x16x16_f16_b"})
                                        for ax2_1, ax3_1 in T.grid(T.int64(16), T.int64(16)):
                                            with T.block("p1_reindex_shared_wmma.matrix_b"):
                                                v2_i, v3_i = T.axis.remap("SS", [ax2_1, ax3_1])
                                                T.reads(p1_reindex_shared[v0, v1, v2_o * T.int64(16) + v2_i, v3_o * T.int64(16) + v3_i])
                                                T.writes(p1_reindex_shared_wmma_matrix_b[v0, v1, v2_o * T.int64(16) + v2_i, v3_o * T.int64(16) + v3_i])
                                                p1_reindex_shared_wmma_matrix_b[v0, v1, v2_o * T.int64(16) + v2_i, v3_o * T.int64(16) + v3_i] = p1_reindex_shared[v0, v1, v2_o * T.int64(16) + v2_i, v3_o * T.int64(16) + v3_i]
                                for ax2_0_3, ax3_0_3, ax0_2, ax1_2, ax4_0_2, ax2_0_4, ax3_0_4 in T.grid(T.int64(1), T.int64(4), T.int64(1), T.int64(1), T.int64(2), T.int64(2), T.int64(1)):
                                    with T.block("conv2d_nchw_o"):
                                        v0 = T.axis.reduce(T.int64(1), ax0_2 + ax0_0 + ax0_1)
                                        v1 = T.axis.reduce(T.int64(1), ax1_1 + ax1_2 + ax1_0)
                                        v2_o = T.axis.spatial(T.int64(196), ax2_0_1_ax3_0_1_fused * T.int64(28) + ax2_0_2_ax3_0_2_fused * T.int64(2) + ax2_0_3 * T.int64(2) + ax2_0_4)
                                        v3_o = T.axis.spatial(T.int64(4), ax3_0_4 + ax3_0_3)
                                        v4_o = T.axis.reduce(T.int64(16), ax4_0_0 * T.int64(16) + ax4_0_1 * T.int64(2) + ax4_0_2)
                                        T.reads(pad_temp_reindex_shared_wmma_matrix_a[v2_o * T.int64(16) : v2_o * T.int64(16) + T.int64(16), v4_o * T.int64(16) : v4_o * T.int64(16) + T.int64(16)], p1_reindex_shared_wmma_matrix_b[v0, v1, v4_o * T.int64(16) : v4_o * T.int64(16) + T.int64(16), v3_o * T.int64(16) : v3_o * T.int64(16) + T.int64(16)])
                                        T.writes(conv2d_nchw_reindex_shared_wmma_accumulator[v2_o * T.int64(16) : v2_o * T.int64(16) + T.int64(16), v3_o * T.int64(16) : v3_o * T.int64(16) + T.int64(16)])
                                        T.block_attr({"meta_schedule.auto_tensorize":"wmma_sync_16x16x16_f16f16f16", "meta_schedule.auto_tensorize_init":"wmma_fill_16x16x16_f16", "meta_schedule.thread_extent_high_inclusive":1024, "meta_schedule.thread_extent_low_inclusive":32, "warp_execution":1})
                                        with T.init():
                                            for ax2_1, ax3_1 in T.grid(T.int64(16), T.int64(16)):
                                                with T.block("conv2d_nchw_init"):
                                                    v2_i_init, v3_i_init = T.axis.remap("SS", [ax2_1, ax3_1])
                                                    T.reads()
                                                    T.writes(conv2d_nchw_reindex_shared_wmma_accumulator[v2_o * T.int64(16) + v2_i_init, v3_o * T.int64(16) + v3_i_init])
                                                    conv2d_nchw_reindex_shared_wmma_accumulator[v2_o * T.int64(16) + v2_i_init, v3_o * T.int64(16) + v3_i_init] = T.float16(0)
                                        for ax2_1, ax3_1, ax4_1 in T.grid(T.int64(16), T.int64(16), T.int64(16)):
                                            with T.block("conv2d_nchw"):
                                                v2_i, v3_i, v4_i = T.axis.remap("SSR", [ax2_1, ax3_1, ax4_1])
                                                T.reads(conv2d_nchw_reindex_shared_wmma_accumulator[v2_o * T.int64(16) + v2_i, v3_o * T.int64(16) + v3_i], pad_temp_reindex_shared_wmma_matrix_a[v2_o * T.int64(16) + v2_i, v4_o * T.int64(16) + v4_i], p1_reindex_shared_wmma_matrix_b[v0, v1, v4_o * T.int64(16) + v4_i, v3_o * T.int64(16) + v3_i])
                                                T.writes(conv2d_nchw_reindex_shared_wmma_accumulator[v2_o * T.int64(16) + v2_i, v3_o * T.int64(16) + v3_i])
                                                T.block_attr({"meta_schedule.tiling_structure":"SSSRRSRS"})
                                                conv2d_nchw_reindex_shared_wmma_accumulator[v2_o * T.int64(16) + v2_i, v3_o * T.int64(16) + v3_i] = conv2d_nchw_reindex_shared_wmma_accumulator[v2_o * T.int64(16) + v2_i, v3_o * T.int64(16) + v3_i] + pad_temp_reindex_shared_wmma_matrix_a[v2_o * T.int64(16) + v2_i, v4_o * T.int64(16) + v4_i] * p1_reindex_shared_wmma_matrix_b[v0, v1, v4_o * T.int64(16) + v4_i, v3_o * T.int64(16) + v3_i]
                        for ax0_0, ax1_0 in T.grid(T.int64(2), T.int64(4)):
                            with T.block("conv2d_nchw_reindex_shared_wmma.accumulator_o"):
                                v0_o = T.axis.spatial(T.int64(196), ax2_0_1_ax3_0_1_fused * T.int64(28) + ax2_0_2_ax3_0_2_fused * T.int64(2) + ax0_0)
                                v1_o = T.axis.spatial(T.int64(4), ax1_0)
                                T.reads(conv2d_nchw_reindex_shared_wmma_accumulator[v0_o * T.int64(16) : v0_o * T.int64(16) + T.int64(16), v1_o * T.int64(16) : v1_o * T.int64(16) + T.int64(16)])
                                T.writes(conv2d_nchw_reindex_shared[v0_o * T.int64(16) : v0_o * T.int64(16) + T.int64(16), v1_o * T.int64(16) : v1_o * T.int64(16) + T.int64(16)])
                                T.block_attr({"meta_schedule.auto_tensorize":"wmma_store_16x16x16_f16_shared"})
                                for ax0_1, ax1_1 in T.grid(T.int64(16), T.int64(16)):
                                    with T.block("conv2d_nchw_reindex_shared_wmma.accumulator"):
                                        v0_i, v1_i = T.axis.remap("SS", [ax0_1, ax1_1])
                                        T.reads(conv2d_nchw_reindex_shared_wmma_accumulator[v0_o * T.int64(16) + v0_i, v1_o * T.int64(16) + v1_i])
                                        T.writes(conv2d_nchw_reindex_shared[v0_o * T.int64(16) + v0_i, v1_o * T.int64(16) + v1_i])
                                        conv2d_nchw_reindex_shared[v0_o * T.int64(16) + v0_i, v1_o * T.int64(16) + v1_i] = conv2d_nchw_reindex_shared_wmma_accumulator[v0_o * T.int64(16) + v0_i, v1_o * T.int64(16) + v1_i]
                    for ax0_ax1_fused in T.serial(T.int64(28672)):
                        with T.block("conv2d_nchw_reindex_shared"):
                            v0 = T.axis.spatial(T.int64(3136), ax2_0_1_ax3_0_1_fused * T.int64(448) + ax0_ax1_fused // T.int64(64))
                            v1 = T.axis.spatial(T.int64(64), ax0_ax1_fused % T.int64(64))
                            T.reads(conv2d_nchw_reindex_shared[v0, v1], p2[v0 // T.int64(3136), v1, T.int64(0), T.int64(0)])
                            T.writes(T_relu[v0 // T.int64(3136), v1, v0 // T.int64(56), v0 % T.int64(56)])
                            T.block_attr({"meta_schedule.cooperative_fetch":1})
                            T_relu[v0 // T.int64(3136), v1, v0 // T.int64(56), v0 % T.int64(56)] = T.max(conv2d_nchw_reindex_shared[v0, v1] + p2[v0 // T.int64(3136), v1, T.int64(0), T.int64(0)], T.float16(0))
    

b0 = sch.get_block(name="pad_temp", func_name="main")
b1 = sch.get_block(name="conv2d_nchw", func_name="main")
b2 = sch.get_block(name="T_add", func_name="main")
b3 = sch.get_block(name="T_relu", func_name="main")
b4 = sch.get_block(name="root", func_name="main")
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.tiling_structure", ann_val="SSSRRSRS")
b5 = sch.reindex(block=b1, buffer=("write", 0))
b6 = sch.reindex(block=b1, buffer=("read", 0))
b7 = sch.reindex(block=b1, buffer=("read", 1))
sch.transform_layout(block=b1, buffer=("read", 0), index_map=lambda v_nn, v_yy, v_xx, v_rc: ((((v_nn*(int64)3136) + (v_yy*(int64)56)) + v_xx), v_rc,), pad_value=None)
sch.transform_layout(block=b1, buffer=("read", 1), index_map=lambda v_ff, v_rc, v_ry, v_rx: (v_ry, v_rx, v_rc, v_ff,), pad_value=None)
sch.transform_layout(block=b1, buffer=("write", 0), index_map=lambda v_nn, v_ff, v_yy, v_xx: ((((v_nn*(int64)3136) + (v_yy*(int64)56)) + v_xx), v_ff,), pad_value=None)
sch.transform_block_layout(block=b5, index_map=lambda v_nn, v_ff, v_yy, v_xx: ((((v_nn*(int64)3136) + (v_yy*(int64)56)) + v_xx), v_ff,))
sch.transform_block_layout(block=b6, index_map=lambda v_nn, v_yy, v_xx, v_rc: ((((v_nn*(int64)3136) + (v_yy*(int64)56)) + v_xx), v_rc,))
sch.transform_block_layout(block=b7, index_map=lambda v_ff, v_rc, v_ry, v_rx: (v_ry, v_rx, v_rc, v_ff,))
sch.transform_block_layout(block=b1, index_map=lambda v_nn, v_ff, v_yy, v_xx, v_rc, v_ry, v_rx: (v_ry, v_rx, (((v_nn*(int64)3136) + (v_yy*(int64)56)) + v_xx), v_ff, v_rc,))
l8, l9, l10, l11, l12 = sch.get_loops(block=b1)
l13, l14 = sch.split(loop=l12, factors=[None, 16], preserve_unit_iters=True)
l15, l16 = sch.split(loop=l11, factors=[None, 16], preserve_unit_iters=True)
l17, l18 = sch.split(loop=l10, factors=[None, 16], preserve_unit_iters=True)
l19, l20, l21, l22, l23, l24, l25, l26 = sch.get_loops(block=b1)
sch.reorder(l23, l25, l18, l16, l14)
b27 = sch.blockize(loop=l18, preserve_unit_iters=True)
sch.annotate(block_or_loop=b27, ann_key="meta_schedule.auto_tensorize", ann_val="wmma_sync_16x16x16_f16f16f16")
sch.annotate(block_or_loop=b27, ann_key="meta_schedule.auto_tensorize_init", ann_val="wmma_fill_16x16x16_f16")
sch.annotate(block_or_loop=b27, ann_key="warp_execution", ann_val=1)
l28, l29, l30, l31, l32 = sch.get_loops(block=b27)
v33, v34, v35 = sch.sample_perfect_tile(loop=l28, n=3, max_innermost_factor=4, decision=[1, 1, 1])
l36, l37, l38 = sch.split(loop=l28, factors=[v33, v34, v35], preserve_unit_iters=True)
v39, v40, v41 = sch.sample_perfect_tile(loop=l29, n=3, max_innermost_factor=4, decision=[1, 1, 1])
l42, l43, l44 = sch.split(loop=l29, factors=[v39, v40, v41], preserve_unit_iters=True)
v45, v46, v47, v48, v49 = sch.sample_perfect_tile(loop=l30, n=5, max_innermost_factor=4, decision=[1, 7, 14, 1, 2])
l50, l51, l52, l53, l54 = sch.split(loop=l30, factors=[v45, v46, v47, v48, v49], preserve_unit_iters=True)
v55, v56, v57, v58, v59 = sch.sample_perfect_tile(loop=l31, n=5, max_innermost_factor=4, decision=[1, 1, 1, 4, 1])
l60, l61, l62, l63, l64 = sch.split(loop=l31, factors=[v55, v56, v57, v58, v59], preserve_unit_iters=True)
v65, v66, v67 = sch.sample_perfect_tile(loop=l32, n=3, max_innermost_factor=4, decision=[1, 8, 2])
l68, l69, l70 = sch.split(loop=l32, factors=[v65, v66, v67], preserve_unit_iters=True)
sch.reorder(l50, l60, l51, l61, l52, l62, l36, l42, l68, l37, l43, l69, l53, l63, l38, l44, l70, l54, l64)
l71 = sch.fuse(l50, l60, preserve_unit_iters=True)
sch.bind(loop=l71, thread_axis="blockIdx.y")
l72 = sch.fuse(l51, l61, preserve_unit_iters=True)
sch.bind(loop=l72, thread_axis="blockIdx.x")
l73 = sch.fuse(l52, l62, preserve_unit_iters=True)
sch.bind(loop=l73, thread_axis="threadIdx.y")
sch.annotate(block_or_loop=b27, ann_key="meta_schedule.thread_extent_low_inclusive", ann_val=32)
sch.annotate(block_or_loop=b27, ann_key="meta_schedule.thread_extent_high_inclusive", ann_val=1024)
b74 = sch.cache_write(block=b27, write_buffer_index=0, storage_scope="shared")
sch.reverse_compute_at(block=b74, loop=l72, preserve_unit_loops=True, index=-1)
b75 = sch.cache_write(block=b27, write_buffer_index=0, storage_scope="wmma.accumulator")
sch.reverse_compute_at(block=b75, loop=l73, preserve_unit_loops=True, index=-1)
l76, l77, l78, l79 = sch.get_loops(block=b74)
l80 = sch.fuse(l78, l79, preserve_unit_iters=True)
v81 = sch.sample_categorical(candidates=[1, 2, 4, 8], probs=[0.25, 0.25, 0.25, 0.25], decision=0)
sch.annotate(block_or_loop=b74, ann_key="meta_schedule.cooperative_fetch", ann_val=v81)
sch.reverse_compute_inline(block=b5)
l82, l83, l84, l85, l86 = sch.get_loops(block=b75)
l87, l88 = sch.split(loop=l86, factors=[None, 16], preserve_unit_iters=True)
l89, l90 = sch.split(loop=l85, factors=[None, 16], preserve_unit_iters=True)
l91, l92, l93, l94, l95, l96, l97 = sch.get_loops(block=b75)
sch.reorder(l96, l90, l88)
b98 = sch.blockize(loop=l90, preserve_unit_iters=True)
sch.annotate(block_or_loop=b98, ann_key="meta_schedule.auto_tensorize", ann_val="wmma_store_16x16x16_f16_shared")
b99 = sch.cache_read(block=b27, read_buffer_index=0, storage_scope="shared", consumer_blocks=[b27])
sch.compute_at(block=b99, loop=l68, preserve_unit_loops=True, index=-1)
l100, l101, l102, l103, l104, l105, l106, l107 = sch.get_loops(block=b99)
l108 = sch.fuse(l106, l107, preserve_unit_iters=True)
v109 = sch.sample_categorical(candidates=[1, 2, 4, 8], probs=[0.25, 0.25, 0.25, 0.25], decision=3)
sch.annotate(block_or_loop=b99, ann_key="meta_schedule.cooperative_fetch", ann_val=v109)
b110 = sch.cache_read(block=b27, read_buffer_index=1, storage_scope="shared", consumer_blocks=[b27])
sch.compute_at(block=b110, loop=l68, preserve_unit_loops=True, index=-1)
l111, l112, l113, l114, l115, l116, l117, l118, l119, l120 = sch.get_loops(block=b110)
l121 = sch.fuse(l117, l118, l119, l120, preserve_unit_iters=True)
v122 = sch.sample_categorical(candidates=[1, 2, 4, 8], probs=[0.25, 0.25, 0.25, 0.25], decision=2)
sch.annotate(block_or_loop=b110, ann_key="meta_schedule.cooperative_fetch", ann_val=v122)
b123 = sch.cache_read(block=b27, read_buffer_index=0, storage_scope="wmma.matrix_a")
sch.compute_at(block=b123, loop=l69, preserve_unit_loops=True, index=-1)
l124, l125, l126, l127, l128, l129, l130, l131, l132, l133, l134 = sch.get_loops(block=b123)
l135, l136 = sch.split(loop=l134, factors=[None, 16], preserve_unit_iters=True)
l137, l138 = sch.split(loop=l133, factors=[None, 16], preserve_unit_iters=True)
l139, l140, l141, l142, l143, l144, l145, l146, l147, l148, l149, l150, l151 = sch.get_loops(block=b123)
sch.reorder(l150, l138, l136)
b152 = sch.blockize(loop=l138, preserve_unit_iters=True)
sch.annotate(block_or_loop=b152, ann_key="meta_schedule.auto_tensorize", ann_val="wmma_load_16x16x16_f16_a")
b153 = sch.cache_read(block=b27, read_buffer_index=1, storage_scope="wmma.matrix_b")
sch.compute_at(block=b153, loop=l69, preserve_unit_loops=True, index=-1)
l154, l155, l156, l157, l158, l159, l160, l161, l162, l163, l164, l165, l166 = sch.get_loops(block=b153)
l167, l168 = sch.split(loop=l166, factors=[None, 16], preserve_unit_iters=True)
l169, l170 = sch.split(loop=l165, factors=[None, 16], preserve_unit_iters=True)
l171, l172, l173, l174, l175, l176, l177, l178, l179, l180, l181, l182, l183, l184, l185 = sch.get_loops(block=b153)
sch.reorder(l184, l170, l168)
b186 = sch.blockize(loop=l170, preserve_unit_iters=True)
sch.annotate(block_or_loop=b186, ann_key="meta_schedule.auto_tensorize", ann_val="wmma_load_16x16x16_f16_b")
sch.compute_inline(block=b6)
sch.compute_inline(block=b7)
sch.storage_align(block=b99, buffer_index=0, axis=-2, factor=32, offset=8)
sch.storage_align(block=b110, buffer_index=0, axis=-2, factor=32, offset=8)
sch.reverse_compute_inline(block=b3)
sch.reverse_compute_inline(block=b2)
sch.compute_inline(block=b0)
v187 = sch.sample_categorical(candidates=[0, 16, 64, 512, 1024], probs=[0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001], decision=0)
sch.annotate(block_or_loop=b4, ann_key="meta_schedule.unroll_explicit", ann_val=v187)
2023-04-11 10:46:37 [INFO] [task_scheduler.cc:170] Design space #1:
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(p0: T.Buffer[(T.int64(1), T.int64(256), T.int64(56), T.int64(56)), "float16"], p1: T.Buffer[(T.int64(64), T.int64(256), T.int64(1), T.int64(1)), "float16"], p2: T.Buffer[(T.int64(1), T.int64(64), T.int64(1), T.int64(1)), "float16"], T_relu: T.Buffer[(T.int64(1), T.int64(64), T.int64(56), T.int64(56)), "float16"]):
        # function attr dict
        T.func_attr({"global_symbol": "main", "tir.noalias": True})
        # body
        with T.block("root"):
            T.reads()
            T.writes()
            T.block_attr({"meta_schedule.unroll_explicit":1024})
            conv2d_nchw_reindex_shared = T.alloc_buffer([T.int64(3136), T.int64(64)], dtype="float16", scope="shared")
            conv2d_nchw_reindex_shared_wmma_accumulator = T.alloc_buffer([T.int64(3136), T.int64(64)], dtype="float16", scope="wmma.accumulator")
            pad_temp_reindex_shared = T.alloc_buffer([T.int64(3136), T.int64(256)], dtype="float16", scope="shared")
            p1_reindex_shared = T.alloc_buffer([T.int64(1), T.int64(1), T.int64(64), T.int64(256)], dtype="float16", scope="shared")
            pad_temp_reindex_shared_wmma_matrix_a = T.alloc_buffer([T.int64(3136), T.int64(256)], dtype="float16", scope="wmma.matrix_a")
            p1_reindex_shared_wmma_matrix_b = T.alloc_buffer([T.int64(1), T.int64(1), T.int64(64), T.int64(256)], dtype="float16", scope="wmma.matrix_b")
            for ax2_0_0_ax3_0_0_fused in T.thread_binding(T.int64(2), thread="blockIdx.y"):
                for ax2_0_1_ax3_0_1_fused in T.thread_binding(T.int64(7), thread="blockIdx.x"):
                    for ax2_0_2_ax3_0_2_fused in T.thread_binding(T.int64(14), thread="threadIdx.y"):
                        for ax0_0, ax1_0, ax4_0_0 in T.grid(T.int64(1), T.int64(1), T.int64(4)):
                            for ax0_ax1_fused in T.serial(T.int64(14336)):
                                with T.block("pad_temp_reindex_shared"):
                                    v0 = T.axis.spatial(T.int64(3136), ax2_0_0_ax3_0_0_fused * T.int64(1568) + ax2_0_1_ax3_0_1_fused * T.int64(224) + ax0_ax1_fused // T.int64(64))
                                    v1 = T.axis.spatial(T.int64(256), ax4_0_0 * T.int64(64) + ax0_ax1_fused % T.int64(64))
                                    T.reads(p0[v0 // T.int64(3136), v1, v0 // T.int64(56), v0 % T.int64(56)])
                                    T.writes(pad_temp_reindex_shared[v0, v1])
                                    T.block_attr({"buffer_dim_align":[[0, 0, 32, 8]], "meta_schedule.cooperative_fetch":4})
                                    pad_temp_reindex_shared[v0, v1] = p0[v0 // T.int64(3136), v1, v0 // T.int64(56), v0 % T.int64(56)]
                            for ax0_ax1_ax2_ax3_fused in T.serial(T.int64(4096)):
                                with T.block("p1_reindex_shared"):
                                    v0 = T.axis.spatial(T.int64(1), T.int64(0))
                                    v1 = T.axis.spatial(T.int64(1), T.int64(0))
                                    v2 = T.axis.spatial(T.int64(64), ax0_ax1_ax2_ax3_fused // T.int64(64))
                                    v3 = T.axis.spatial(T.int64(256), ax4_0_0 * T.int64(64) + ax0_ax1_ax2_ax3_fused % T.int64(64))
                                    T.reads(p1[v2, v3, v0, v1])
                                    T.writes(p1_reindex_shared[v0, v1, v2, v3])
                                    T.block_attr({"buffer_dim_align":[[0, 2, 32, 8]], "meta_schedule.cooperative_fetch":2})
                                    p1_reindex_shared[v0, v1, v2, v3] = p1[v2, v3, v0, v1]
                            for ax0_1, ax1_1, ax4_0_1 in T.grid(T.int64(1), T.int64(1), T.int64(2)):
                                for ax0_0_1, ax1_0_1 in T.grid(T.int64(2), T.int64(2)):
                                    with T.block("pad_temp_reindex_shared_wmma.matrix_a_o"):
                                        v0_o = T.axis.spatial(T.int64(196), ax2_0_0_ax3_0_0_fused * T.int64(98) + ax2_0_1_ax3_0_1_fused * T.int64(14) + ax2_0_2_ax3_0_2_fused // T.int64(2) * T.int64(2) + ax0_0_1)
                                        v1_o = T.axis.spatial(T.int64(16), ax4_0_0 * T.int64(4) + ax4_0_1 * T.int64(2) + ax1_0_1)
                                        T.reads(pad_temp_reindex_shared[v0_o * T.int64(16) : v0_o * T.int64(16) + T.int64(16), v1_o * T.int64(16) : v1_o * T.int64(16) + T.int64(16)])
                                        T.writes(pad_temp_reindex_shared_wmma_matrix_a[v0_o * T.int64(16) : v0_o * T.int64(16) + T.int64(16), v1_o * T.int64(16) : v1_o * T.int64(16) + T.int64(16)])
                                        T.block_attr({"meta_schedule.auto_tensorize":"wmma_load_16x16x16_f16_a"})
                                        for ax0_1_1, ax1_1_1 in T.grid(T.int64(16), T.int64(16)):
                                            with T.block("pad_temp_reindex_shared_wmma.matrix_a"):
                                                v0_i, v1_i = T.axis.remap("SS", [ax0_1_1, ax1_1_1])
                                                T.reads(pad_temp_reindex_shared[v0_o * T.int64(16) + v0_i, v1_o * T.int64(16) + v1_i])
                                                T.writes(pad_temp_reindex_shared_wmma_matrix_a[v0_o * T.int64(16) + v0_i, v1_o * T.int64(16) + v1_i])
                                                pad_temp_reindex_shared_wmma_matrix_a[v0_o * T.int64(16) + v0_i, v1_o * T.int64(16) + v1_i] = pad_temp_reindex_shared[v0_o * T.int64(16) + v0_i, v1_o * T.int64(16) + v1_i]
                                for ax0, ax1, ax2_0, ax3_0 in T.grid(T.int64(1), T.int64(1), T.int64(2), T.int64(2)):
                                    with T.block("p1_reindex_shared_wmma.matrix_b_o"):
                                        v0, v1 = T.axis.remap("SS", [ax0, ax1])
                                        v2_o = T.axis.spatial(T.int64(4), ax2_0_2_ax3_0_2_fused % T.int64(2) * T.int64(2) + ax2_0)
                                        v3_o = T.axis.spatial(T.int64(16), ax4_0_0 * T.int64(4) + ax4_0_1 * T.int64(2) + ax3_0)
                                        T.reads(p1_reindex_shared[v0, v1, v2_o * T.int64(16) : v2_o * T.int64(16) + T.int64(16), v3_o * T.int64(16) : v3_o * T.int64(16) + T.int64(16)])
                                        T.writes(p1_reindex_shared_wmma_matrix_b[v0, v1, v2_o * T.int64(16) : v2_o * T.int64(16) + T.int64(16), v3_o * T.int64(16) : v3_o * T.int64(16) + T.int64(16)])
                                        T.block_attr({"meta_schedule.auto_tensorize":"wmma_load_16x16x16_f16_b_trans"})
                                        for ax2_1, ax3_1 in T.grid(T.int64(16), T.int64(16)):
                                            with T.block("p1_reindex_shared_wmma.matrix_b"):
                                                v2_i, v3_i = T.axis.remap("SS", [ax2_1, ax3_1])
                                                T.reads(p1_reindex_shared[v0, v1, v2_o * T.int64(16) + v2_i, v3_o * T.int64(16) + v3_i])
                                                T.writes(p1_reindex_shared_wmma_matrix_b[v0, v1, v2_o * T.int64(16) + v2_i, v3_o * T.int64(16) + v3_i])
                                                p1_reindex_shared_wmma_matrix_b[v0, v1, v2_o * T.int64(16) + v2_i, v3_o * T.int64(16) + v3_i] = p1_reindex_shared[v0, v1, v2_o * T.int64(16) + v2_i, v3_o * T.int64(16) + v3_i]
                                for ax2_0_3, ax3_0_3, ax0_2, ax1_2, ax4_0_2, ax2_0_4, ax3_0_4 in T.grid(T.int64(2), T.int64(1), T.int64(1), T.int64(1), T.int64(2), T.int64(1), T.int64(2)):
                                    with T.block("conv2d_nchw_o"):
                                        v0 = T.axis.reduce(T.int64(1), ax0_2 + ax0_0 + ax0_1)
                                        v1 = T.axis.reduce(T.int64(1), ax1_1 + ax1_2 + ax1_0)
                                        v2_o = T.axis.spatial(T.int64(196), ax2_0_0_ax3_0_0_fused * T.int64(98) + ax2_0_1_ax3_0_1_fused * T.int64(14) + ax2_0_2_ax3_0_2_fused // T.int64(2) * T.int64(2) + ax2_0_3 + ax2_0_4)
                                        v3_o = T.axis.spatial(T.int64(4), ax2_0_2_ax3_0_2_fused % T.int64(2) * T.int64(2) + ax3_0_3 * T.int64(2) + ax3_0_4)
                                        v4_o = T.axis.reduce(T.int64(16), ax4_0_0 * T.int64(4) + ax4_0_1 * T.int64(2) + ax4_0_2)
                                        T.reads(pad_temp_reindex_shared_wmma_matrix_a[v2_o * T.int64(16) : v2_o * T.int64(16) + T.int64(16), v4_o * T.int64(16) : v4_o * T.int64(16) + T.int64(16)], p1_reindex_shared_wmma_matrix_b[v0, v1, v3_o * T.int64(16) : v3_o * T.int64(16) + T.int64(16), v4_o * T.int64(16) : v4_o * T.int64(16) + T.int64(16)])
                                        T.writes(conv2d_nchw_reindex_shared_wmma_accumulator[v2_o * T.int64(16) : v2_o * T.int64(16) + T.int64(16), v3_o * T.int64(16) : v3_o * T.int64(16) + T.int64(16)])
                                        T.block_attr({"meta_schedule.auto_tensorize":"wmma_sync_16x16x16_f16f16f16_trans", "meta_schedule.auto_tensorize_init":"wmma_fill_16x16x16_f16", "meta_schedule.thread_extent_high_inclusive":1024, "meta_schedule.thread_extent_low_inclusive":32, "warp_execution":1})
                                        with T.init():
                                            for ax2_1, ax3_1 in T.grid(T.int64(16), T.int64(16)):
                                                with T.block("conv2d_nchw_init"):
                                                    v2_i_init, v3_i_init = T.axis.remap("SS", [ax2_1, ax3_1])
                                                    T.reads()
                                                    T.writes(conv2d_nchw_reindex_shared_wmma_accumulator[v2_o * T.int64(16) + v2_i_init, v3_o * T.int64(16) + v3_i_init])
                                                    conv2d_nchw_reindex_shared_wmma_accumulator[v2_o * T.int64(16) + v2_i_init, v3_o * T.int64(16) + v3_i_init] = T.float16(0)
                                        for ax2_1, ax3_1, ax4_1 in T.grid(T.int64(16), T.int64(16), T.int64(16)):
                                            with T.block("conv2d_nchw"):
                                                v2_i, v3_i, v4_i = T.axis.remap("SSR", [ax2_1, ax3_1, ax4_1])
                                                T.reads(conv2d_nchw_reindex_shared_wmma_accumulator[v2_o * T.int64(16) + v2_i, v3_o * T.int64(16) + v3_i], pad_temp_reindex_shared_wmma_matrix_a[v2_o * T.int64(16) + v2_i, v4_o * T.int64(16) + v4_i], p1_reindex_shared_wmma_matrix_b[v0, v1, v3_o * T.int64(16) + v3_i, v4_o * T.int64(16) + v4_i])
                                                T.writes(conv2d_nchw_reindex_shared_wmma_accumulator[v2_o * T.int64(16) + v2_i, v3_o * T.int64(16) + v3_i])
                                                T.block_attr({"meta_schedule.tiling_structure":"SSSRRSRS"})
                                                conv2d_nchw_reindex_shared_wmma_accumulator[v2_o * T.int64(16) + v2_i, v3_o * T.int64(16) + v3_i] = conv2d_nchw_reindex_shared_wmma_accumulator[v2_o * T.int64(16) + v2_i, v3_o * T.int64(16) + v3_i] + pad_temp_reindex_shared_wmma_matrix_a[v2_o * T.int64(16) + v2_i, v4_o * T.int64(16) + v4_i] * p1_reindex_shared_wmma_matrix_b[v0, v1, v3_o * T.int64(16) + v3_i, v4_o * T.int64(16) + v4_i]
                        for ax0_0, ax1_0 in T.grid(T.int64(2), T.int64(2)):
                            with T.block("conv2d_nchw_reindex_shared_wmma.accumulator_o"):
                                v0_o = T.axis.spatial(T.int64(196), ax2_0_0_ax3_0_0_fused * T.int64(98) + ax2_0_1_ax3_0_1_fused * T.int64(14) + ax2_0_2_ax3_0_2_fused // T.int64(2) * T.int64(2) + ax0_0)
                                v1_o = T.axis.spatial(T.int64(4), ax2_0_2_ax3_0_2_fused % T.int64(2) * T.int64(2) + ax1_0)
                                T.reads(conv2d_nchw_reindex_shared_wmma_accumulator[v0_o * T.int64(16) : v0_o * T.int64(16) + T.int64(16), v1_o * T.int64(16) : v1_o * T.int64(16) + T.int64(16)])
                                T.writes(conv2d_nchw_reindex_shared[v0_o * T.int64(16) : v0_o * T.int64(16) + T.int64(16), v1_o * T.int64(16) : v1_o * T.int64(16) + T.int64(16)])
                                T.block_attr({"meta_schedule.auto_tensorize":"wmma_store_16x16x16_f16_shared"})
                                for ax0_1, ax1_1 in T.grid(T.int64(16), T.int64(16)):
                                    with T.block("conv2d_nchw_reindex_shared_wmma.accumulator"):
                                        v0_i, v1_i = T.axis.remap("SS", [ax0_1, ax1_1])
                                        T.reads(conv2d_nchw_reindex_shared_wmma_accumulator[v0_o * T.int64(16) + v0_i, v1_o * T.int64(16) + v1_i])
                                        T.writes(conv2d_nchw_reindex_shared[v0_o * T.int64(16) + v0_i, v1_o * T.int64(16) + v1_i])
                                        conv2d_nchw_reindex_shared[v0_o * T.int64(16) + v0_i, v1_o * T.int64(16) + v1_i] = conv2d_nchw_reindex_shared_wmma_accumulator[v0_o * T.int64(16) + v0_i, v1_o * T.int64(16) + v1_i]
                    for ax0_ax1_fused in T.serial(T.int64(14336)):
                        with T.block("conv2d_nchw_reindex_shared"):
                            v0 = T.axis.spatial(T.int64(3136), ax2_0_0_ax3_0_0_fused * T.int64(1568) + ax2_0_1_ax3_0_1_fused * T.int64(224) + ax0_ax1_fused // T.int64(64))
                            v1 = T.axis.spatial(T.int64(64), ax0_ax1_fused % T.int64(64))
                            T.reads(conv2d_nchw_reindex_shared[v0, v1], p2[v0 // T.int64(3136), v1, T.int64(0), T.int64(0)])
                            T.writes(T_relu[v0 // T.int64(3136), v1, v0 // T.int64(56), v0 % T.int64(56)])
                            T.block_attr({"meta_schedule.cooperative_fetch":2})
                            T_relu[v0 // T.int64(3136), v1, v0 // T.int64(56), v0 % T.int64(56)] = T.max(conv2d_nchw_reindex_shared[v0, v1] + p2[v0 // T.int64(3136), v1, T.int64(0), T.int64(0)], T.float16(0))
    

b0 = sch.get_block(name="pad_temp", func_name="main")
b1 = sch.get_block(name="conv2d_nchw", func_name="main")
b2 = sch.get_block(name="T_add", func_name="main")
b3 = sch.get_block(name="T_relu", func_name="main")
b4 = sch.get_block(name="root", func_name="main")
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.tiling_structure", ann_val="SSSRRSRS")
b5 = sch.reindex(block=b1, buffer=("write", 0))
b6 = sch.reindex(block=b1, buffer=("read", 0))
b7 = sch.reindex(block=b1, buffer=("read", 1))
sch.transform_layout(block=b1, buffer=("read", 0), index_map=lambda v_nn, v_yy, v_xx, v_rc: ((((v_nn*(int64)3136) + (v_yy*(int64)56)) + v_xx), v_rc,), pad_value=None)
sch.transform_layout(block=b1, buffer=("read", 1), index_map=lambda v_ff, v_rc, v_ry, v_rx: (v_ry, v_rx, v_ff, v_rc,), pad_value=None)
sch.transform_layout(block=b1, buffer=("write", 0), index_map=lambda v_nn, v_ff, v_yy, v_xx: ((((v_nn*(int64)3136) + (v_yy*(int64)56)) + v_xx), v_ff,), pad_value=None)
sch.transform_block_layout(block=b5, index_map=lambda v_nn, v_ff, v_yy, v_xx: ((((v_nn*(int64)3136) + (v_yy*(int64)56)) + v_xx), v_ff,))
sch.transform_block_layout(block=b6, index_map=lambda v_nn, v_yy, v_xx, v_rc: ((((v_nn*(int64)3136) + (v_yy*(int64)56)) + v_xx), v_rc,))
sch.transform_block_layout(block=b7, index_map=lambda v_ff, v_rc, v_ry, v_rx: (v_ry, v_rx, v_ff, v_rc,))
sch.transform_block_layout(block=b1, index_map=lambda v_nn, v_ff, v_yy, v_xx, v_rc, v_ry, v_rx: (v_ry, v_rx, (((v_nn*(int64)3136) + (v_yy*(int64)56)) + v_xx), v_ff, v_rc,))
l8, l9, l10, l11, l12 = sch.get_loops(block=b1)
l13, l14 = sch.split(loop=l12, factors=[None, 16], preserve_unit_iters=True)
l15, l16 = sch.split(loop=l11, factors=[None, 16], preserve_unit_iters=True)
l17, l18 = sch.split(loop=l10, factors=[None, 16], preserve_unit_iters=True)
l19, l20, l21, l22, l23, l24, l25, l26 = sch.get_loops(block=b1)
sch.reorder(l23, l25, l18, l16, l14)
b27 = sch.blockize(loop=l18, preserve_unit_iters=True)
sch.annotate(block_or_loop=b27, ann_key="meta_schedule.auto_tensorize", ann_val="wmma_sync_16x16x16_f16f16f16_trans")
sch.annotate(block_or_loop=b27, ann_key="meta_schedule.auto_tensorize_init", ann_val="wmma_fill_16x16x16_f16")
sch.annotate(block_or_loop=b27, ann_key="warp_execution", ann_val=1)
l28, l29, l30, l31, l32 = sch.get_loops(block=b27)
v33, v34, v35 = sch.sample_perfect_tile(loop=l28, n=3, max_innermost_factor=4, decision=[1, 1, 1])
l36, l37, l38 = sch.split(loop=l28, factors=[v33, v34, v35], preserve_unit_iters=True)
v39, v40, v41 = sch.sample_perfect_tile(loop=l29, n=3, max_innermost_factor=4, decision=[1, 1, 1])
l42, l43, l44 = sch.split(loop=l29, factors=[v39, v40, v41], preserve_unit_iters=True)
v45, v46, v47, v48, v49 = sch.sample_perfect_tile(loop=l30, n=5, max_innermost_factor=4, decision=[2, 7, 7, 2, 1])
l50, l51, l52, l53, l54 = sch.split(loop=l30, factors=[v45, v46, v47, v48, v49], preserve_unit_iters=True)
v55, v56, v57, v58, v59 = sch.sample_perfect_tile(loop=l31, n=5, max_innermost_factor=4, decision=[1, 1, 2, 1, 2])
l60, l61, l62, l63, l64 = sch.split(loop=l31, factors=[v55, v56, v57, v58, v59], preserve_unit_iters=True)
v65, v66, v67 = sch.sample_perfect_tile(loop=l32, n=3, max_innermost_factor=4, decision=[4, 2, 2])
l68, l69, l70 = sch.split(loop=l32, factors=[v65, v66, v67], preserve_unit_iters=True)
sch.reorder(l50, l60, l51, l61, l52, l62, l36, l42, l68, l37, l43, l69, l53, l63, l38, l44, l70, l54, l64)
l71 = sch.fuse(l50, l60, preserve_unit_iters=True)
sch.bind(loop=l71, thread_axis="blockIdx.y")
l72 = sch.fuse(l51, l61, preserve_unit_iters=True)
sch.bind(loop=l72, thread_axis="blockIdx.x")
l73 = sch.fuse(l52, l62, preserve_unit_iters=True)
sch.bind(loop=l73, thread_axis="threadIdx.y")
sch.annotate(block_or_loop=b27, ann_key="meta_schedule.thread_extent_low_inclusive", ann_val=32)
sch.annotate(block_or_loop=b27, ann_key="meta_schedule.thread_extent_high_inclusive", ann_val=1024)
b74 = sch.cache_write(block=b27, write_buffer_index=0, storage_scope="shared")
sch.reverse_compute_at(block=b74, loop=l72, preserve_unit_loops=True, index=-1)
b75 = sch.cache_write(block=b27, write_buffer_index=0, storage_scope="wmma.accumulator")
sch.reverse_compute_at(block=b75, loop=l73, preserve_unit_loops=True, index=-1)
l76, l77, l78, l79 = sch.get_loops(block=b74)
l80 = sch.fuse(l78, l79, preserve_unit_iters=True)
v81 = sch.sample_categorical(candidates=[1, 2, 4, 8], probs=[0.25, 0.25, 0.25, 0.25], decision=1)
sch.annotate(block_or_loop=b74, ann_key="meta_schedule.cooperative_fetch", ann_val=v81)
sch.reverse_compute_inline(block=b5)
l82, l83, l84, l85, l86 = sch.get_loops(block=b75)
l87, l88 = sch.split(loop=l86, factors=[None, 16], preserve_unit_iters=True)
l89, l90 = sch.split(loop=l85, factors=[None, 16], preserve_unit_iters=True)
l91, l92, l93, l94, l95, l96, l97 = sch.get_loops(block=b75)
sch.reorder(l96, l90, l88)
b98 = sch.blockize(loop=l90, preserve_unit_iters=True)
sch.annotate(block_or_loop=b98, ann_key="meta_schedule.auto_tensorize", ann_val="wmma_store_16x16x16_f16_shared")
b99 = sch.cache_read(block=b27, read_buffer_index=0, storage_scope="shared", consumer_blocks=[b27])
sch.compute_at(block=b99, loop=l68, preserve_unit_loops=True, index=-1)
l100, l101, l102, l103, l104, l105, l106, l107 = sch.get_loops(block=b99)
l108 = sch.fuse(l106, l107, preserve_unit_iters=True)
v109 = sch.sample_categorical(candidates=[1, 2, 4, 8], probs=[0.25, 0.25, 0.25, 0.25], decision=2)
sch.annotate(block_or_loop=b99, ann_key="meta_schedule.cooperative_fetch", ann_val=v109)
b110 = sch.cache_read(block=b27, read_buffer_index=1, storage_scope="shared", consumer_blocks=[b27])
sch.compute_at(block=b110, loop=l68, preserve_unit_loops=True, index=-1)
l111, l112, l113, l114, l115, l116, l117, l118, l119, l120 = sch.get_loops(block=b110)
l121 = sch.fuse(l117, l118, l119, l120, preserve_unit_iters=True)
v122 = sch.sample_categorical(candidates=[1, 2, 4, 8], probs=[0.25, 0.25, 0.25, 0.25], decision=1)
sch.annotate(block_or_loop=b110, ann_key="meta_schedule.cooperative_fetch", ann_val=v122)
b123 = sch.cache_read(block=b27, read_buffer_index=0, storage_scope="wmma.matrix_a")
sch.compute_at(block=b123, loop=l69, preserve_unit_loops=True, index=-1)
l124, l125, l126, l127, l128, l129, l130, l131, l132, l133, l134 = sch.get_loops(block=b123)
l135, l136 = sch.split(loop=l134, factors=[None, 16], preserve_unit_iters=True)
l137, l138 = sch.split(loop=l133, factors=[None, 16], preserve_unit_iters=True)
l139, l140, l141, l142, l143, l144, l145, l146, l147, l148, l149, l150, l151 = sch.get_loops(block=b123)
sch.reorder(l150, l138, l136)
b152 = sch.blockize(loop=l138, preserve_unit_iters=True)
sch.annotate(block_or_loop=b152, ann_key="meta_schedule.auto_tensorize", ann_val="wmma_load_16x16x16_f16_a")
b153 = sch.cache_read(block=b27, read_buffer_index=1, storage_scope="wmma.matrix_b")
sch.compute_at(block=b153, loop=l69, preserve_unit_loops=True, index=-1)
l154, l155, l156, l157, l158, l159, l160, l161, l162, l163, l164, l165, l166 = sch.get_loops(block=b153)
l167, l168 = sch.split(loop=l166, factors=[None, 16], preserve_unit_iters=True)
l169, l170 = sch.split(loop=l165, factors=[None, 16], preserve_unit_iters=True)
l171, l172, l173, l174, l175, l176, l177, l178, l179, l180, l181, l182, l183, l184, l185 = sch.get_loops(block=b153)
sch.reorder(l184, l170, l168)
b186 = sch.blockize(loop=l170, preserve_unit_iters=True)
sch.annotate(block_or_loop=b186, ann_key="meta_schedule.auto_tensorize", ann_val="wmma_load_16x16x16_f16_b_trans")
sch.compute_inline(block=b6)
sch.compute_inline(block=b7)
sch.storage_align(block=b99, buffer_index=0, axis=-2, factor=32, offset=8)
sch.storage_align(block=b110, buffer_index=0, axis=-2, factor=32, offset=8)
sch.reverse_compute_inline(block=b3)
sch.reverse_compute_inline(block=b2)
sch.compute_inline(block=b0)
v187 = sch.sample_categorical(candidates=[0, 16, 64, 512, 1024], probs=[0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001], decision=4)
sch.annotate(block_or_loop=b4, ann_key="meta_schedule.unroll_explicit", ann_val=v187)
